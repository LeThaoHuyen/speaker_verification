{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# config\n",
    "config = { \n",
    "    \"dataroot\": \"./data\",\n",
    "    \"n_triplets\": 100000, #1000000,\n",
    "    \"embedding_size\": 512,\n",
    "    \"batch_size\": 512,\n",
    "    \"test_batch_size\":  128,\n",
    "    \"lr\": 0.1,\n",
    "    \"optimizer\": \"adagrad\",\n",
    "    \"wd\": 0.0,\n",
    "    \"lr_decay\": 1e-4,\n",
    "    \"cuda\": True,\n",
    "    \"start_epoch\": 1,\n",
    "    \"epochs\": 1,\n",
    "    \"min_softmax_epoch\": 2,\n",
    "    \"margin\": 0.1, # or alpha in triplet loss\n",
    "    \"log_interval\": 1,\n",
    "    \"loss_ratio\": 2.0, \n",
    "    \"test_input_per_file\": 8,\n",
    "}\n",
    "\n",
    "from easydict import EasyDict\n",
    "config = EasyDict(config)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepSpeaker model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/anhpn19/anaconda3/envs/hh_env/lib/python3.9/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "from torch.autograd import Function\n",
    "\n",
    "\n",
    "class PairwiseDistance(Function):\n",
    "    def __init__(self, p):\n",
    "        super(PairwiseDistance, self).__init__()\n",
    "        self.norm = p\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        assert x1.size() == x2.size()\n",
    "        eps = 1e-4 / x1.size(1)\n",
    "        diff = torch.abs(x1 - x2)\n",
    "        out = torch.pow(diff, self.norm).sum(dim=1)\n",
    "        return torch.pow(out + eps, 1. / self.norm)\n",
    "\n",
    "class TripletMarginLoss(Function):\n",
    "    \"\"\"Triplet loss function.\n",
    "    \"\"\"\n",
    "    def __init__(self, margin):\n",
    "        super(TripletMarginLoss, self).__init__()\n",
    "        self.margin = margin\n",
    "        self.pdist = PairwiseDistance(2)  # norm 2\n",
    "\n",
    "    def forward(self, anchor, positive, negative):\n",
    "        d_p = self.pdist.forward(anchor, positive)\n",
    "        d_n = self.pdist.forward(anchor, negative)\n",
    "\n",
    "        dist_hinge = torch.clamp(self.margin + d_p - d_n, min=0.0)\n",
    "        loss = torch.mean(dist_hinge)\n",
    "        return loss\n",
    "\n",
    "\n",
    "class ReLU(nn.Hardtanh):\n",
    "\n",
    "    def __init__(self, inplace=False):\n",
    "        super(ReLU, self).__init__(0, 20, inplace)\n",
    "\n",
    "    def __repr__(self):\n",
    "        inplace_str = 'inplace' if self.inplace else ''\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "            + inplace_str + ')'\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1):\n",
    "    \"3x3 convolution with padding\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=1, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "\n",
    "        out += residual\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class myResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000):\n",
    "\n",
    "        super(myResNet, self).__init__()\n",
    "\n",
    "        self.relu = ReLU(inplace=True)\n",
    "        self.inplanes = 64\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=2, padding=2,bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.layer1 = self._make_layer(block, 64, layers[0])\n",
    "\n",
    "        self.inplanes = 128\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=5, stride=2, padding=2,bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(128)\n",
    "        self.layer2 = self._make_layer(block, 128, layers[1])\n",
    "\n",
    "        self.inplanes = 256\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=5, stride=2, padding=2,bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(256)\n",
    "        self.layer3 = self._make_layer(block, 256, layers[2])\n",
    "        \n",
    "        self.inplanes = 512\n",
    "        self.conv4 = nn.Conv2d(256, 512, kernel_size=5, stride=2, padding=2,bias=False)\n",
    "        self.bn4 = nn.BatchNorm2d(512)\n",
    "        self.layer4 = self._make_layer(block, 512, layers[3])\n",
    "\n",
    "        \n",
    "        # self.avgpool = nn.AdaptiveAvgPool2d((1,None))\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((None,1))\n",
    "    \n",
    "        self.fc = nn.Linear(512 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
    "                m.weight.data.normal_(0, math.sqrt(2. / n))\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1):\n",
    "\n",
    "        layers = []\n",
    "        layers.append(block(self.inplanes, planes, stride))\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for i in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    # def forward(self, x):\n",
    "    #     x = self.conv1(x)\n",
    "    #     x = self.bn1(x)\n",
    "    #     x = self.relu(x)\n",
    "    #     x = self.maxpool(x)\n",
    "\n",
    "    #     x = self.layer1(x)\n",
    "    #     x = self.layer2(x)\n",
    "    #     x = self.layer3(x)\n",
    "    #     x = self.layer4(x)\n",
    "\n",
    "    #     x = self.avgpool(x)\n",
    "    #     x = x.view(x.size(0), -1)\n",
    "    #     x = self.fc(x)\n",
    "\n",
    "    #     return x\n",
    "\n",
    "class DeepSpeakerModel(nn.Module):\n",
    "    def __init__(self,embedding_size,num_classes,feature_dim = 64):\n",
    "        super(DeepSpeakerModel, self).__init__()\n",
    "\n",
    "        self.embedding_size = embedding_size\n",
    "\n",
    "        self.model = myResNet(BasicBlock, [1, 1, 1, 1])\n",
    "        if feature_dim == 64:\n",
    "            self.model.fc = nn.Linear(512*4, self.embedding_size)\n",
    "            print(\"Hi I'm tired\")\n",
    "        elif feature_dim == 40:\n",
    "            self.model.fc = nn.Linear(256 * 5, self.embedding_size)\n",
    "        self.model.classifier = nn.Linear(self.embedding_size, num_classes)\n",
    "\n",
    "\n",
    "\n",
    "    def l2_norm(self,input):\n",
    "        input_size = input.size()\n",
    "        buffer = torch.pow(input, 2)\n",
    "\n",
    "        normp = torch.sum(buffer, 1).add_(1e-10)\n",
    "        norm = torch.sqrt(normp)\n",
    "\n",
    "        _output = torch.div(input, norm.view(-1, 1).expand_as(input))\n",
    "\n",
    "        output = _output.view(input_size)\n",
    "\n",
    "        return output\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.layer1(x)\n",
    "\n",
    "        x = self.model.conv2(x)\n",
    "        x = self.model.bn2(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.layer2(x)\n",
    "\n",
    "        x = self.model.conv3(x)\n",
    "        x = self.model.bn3(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.layer3(x)\n",
    "\n",
    "        x = self.model.conv4(x)\n",
    "        x = self.model.bn4(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        x = self.model.avgpool(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "\n",
    "        # print(x.size())\n",
    "        x = self.model.fc(x)\n",
    "       \n",
    "        self.features = self.l2_norm(x)\n",
    "        # Multiply by alpha = 10 as suggested in https://arxiv.org/pdf/1703.09507.pdf\n",
    "        alpha=10\n",
    "        self.features = self.features*alpha\n",
    "\n",
    "        #x = x.resize(int(x.size(0) / 17),17 , 512)\n",
    "        #self.features =torch.mean(x,dim=1)\n",
    "        #x = self.model.classifier(self.features)\n",
    "        return self.features\n",
    "\n",
    "    def forward_classifier(self, x):\n",
    "        features = self.forward(x)\n",
    "        res = self.model.classifier(features)\n",
    "        return res\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "train_csv = pd.read_csv(\"./data/train_data.csv\")\n",
    "train_csv = train_csv[train_csv[\"is_audio\"] == True]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>test_or_train</th>\n",
       "      <th>dialect_region</th>\n",
       "      <th>speaker_id</th>\n",
       "      <th>filename</th>\n",
       "      <th>path_from_data_dir</th>\n",
       "      <th>path_from_data_dir_windows</th>\n",
       "      <th>is_converted_audio</th>\n",
       "      <th>is_audio</th>\n",
       "      <th>is_word_file</th>\n",
       "      <th>is_phonetic_file</th>\n",
       "      <th>is_sentence_file</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR4</td>\n",
       "      <td>MMDM0</td>\n",
       "      <td>SI681.WAV.wav</td>\n",
       "      <td>TRAIN/DR4/MMDM0/SI681.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SI681.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR4</td>\n",
       "      <td>MMDM0</td>\n",
       "      <td>SI1311.WAV.wav</td>\n",
       "      <td>TRAIN/DR4/MMDM0/SI1311.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SI1311.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>8.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR4</td>\n",
       "      <td>MMDM0</td>\n",
       "      <td>SI681.WAV</td>\n",
       "      <td>TRAIN/DR4/MMDM0/SI681.WAV</td>\n",
       "      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SI681.WAV</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>11.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR4</td>\n",
       "      <td>MMDM0</td>\n",
       "      <td>SX141.WAV.wav</td>\n",
       "      <td>TRAIN/DR4/MMDM0/SX141.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SX141.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>14.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR4</td>\n",
       "      <td>MMDM0</td>\n",
       "      <td>SX51.WAV.wav</td>\n",
       "      <td>TRAIN/DR4/MMDM0/SX51.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR4\\\\MMDM0\\\\SX51.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23088</th>\n",
       "      <td>23089.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR8</td>\n",
       "      <td>MRDM0</td>\n",
       "      <td>SA1.WAV.wav</td>\n",
       "      <td>TRAIN/DR8/MRDM0/SA1.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR8\\\\MRDM0\\\\SA1.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23095</th>\n",
       "      <td>23096.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR8</td>\n",
       "      <td>MRDM0</td>\n",
       "      <td>SI1044.WAV.wav</td>\n",
       "      <td>TRAIN/DR8/MRDM0/SI1044.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR8\\\\MRDM0\\\\SI1044.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23096</th>\n",
       "      <td>23097.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR8</td>\n",
       "      <td>MRDM0</td>\n",
       "      <td>SX245.WAV</td>\n",
       "      <td>TRAIN/DR8/MRDM0/SX245.WAV</td>\n",
       "      <td>TRAIN\\\\DR8\\\\MRDM0\\\\SX245.WAV</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23097</th>\n",
       "      <td>23098.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR8</td>\n",
       "      <td>MRDM0</td>\n",
       "      <td>SA2.WAV.wav</td>\n",
       "      <td>TRAIN/DR8/MRDM0/SA2.WAV.wav</td>\n",
       "      <td>TRAIN\\\\DR8\\\\MRDM0\\\\SA2.WAV.wav</td>\n",
       "      <td>True</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23098</th>\n",
       "      <td>23099.0</td>\n",
       "      <td>TRAIN</td>\n",
       "      <td>DR8</td>\n",
       "      <td>MRDM0</td>\n",
       "      <td>SX335.WAV</td>\n",
       "      <td>TRAIN/DR8/MRDM0/SX335.WAV</td>\n",
       "      <td>TRAIN\\\\DR8\\\\MRDM0\\\\SX335.WAV</td>\n",
       "      <td>False</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9240 rows Ã— 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         index test_or_train dialect_region speaker_id        filename  \\\n",
       "0          1.0         TRAIN            DR4      MMDM0   SI681.WAV.wav   \n",
       "6          7.0         TRAIN            DR4      MMDM0  SI1311.WAV.wav   \n",
       "7          8.0         TRAIN            DR4      MMDM0       SI681.WAV   \n",
       "10        11.0         TRAIN            DR4      MMDM0   SX141.WAV.wav   \n",
       "13        14.0         TRAIN            DR4      MMDM0    SX51.WAV.wav   \n",
       "...        ...           ...            ...        ...             ...   \n",
       "23088  23089.0         TRAIN            DR8      MRDM0     SA1.WAV.wav   \n",
       "23095  23096.0         TRAIN            DR8      MRDM0  SI1044.WAV.wav   \n",
       "23096  23097.0         TRAIN            DR8      MRDM0       SX245.WAV   \n",
       "23097  23098.0         TRAIN            DR8      MRDM0     SA2.WAV.wav   \n",
       "23098  23099.0         TRAIN            DR8      MRDM0       SX335.WAV   \n",
       "\n",
       "                   path_from_data_dir         path_from_data_dir_windows  \\\n",
       "0       TRAIN/DR4/MMDM0/SI681.WAV.wav   TRAIN\\\\DR4\\\\MMDM0\\\\SI681.WAV.wav   \n",
       "6      TRAIN/DR4/MMDM0/SI1311.WAV.wav  TRAIN\\\\DR4\\\\MMDM0\\\\SI1311.WAV.wav   \n",
       "7           TRAIN/DR4/MMDM0/SI681.WAV       TRAIN\\\\DR4\\\\MMDM0\\\\SI681.WAV   \n",
       "10      TRAIN/DR4/MMDM0/SX141.WAV.wav   TRAIN\\\\DR4\\\\MMDM0\\\\SX141.WAV.wav   \n",
       "13       TRAIN/DR4/MMDM0/SX51.WAV.wav    TRAIN\\\\DR4\\\\MMDM0\\\\SX51.WAV.wav   \n",
       "...                               ...                                ...   \n",
       "23088     TRAIN/DR8/MRDM0/SA1.WAV.wav     TRAIN\\\\DR8\\\\MRDM0\\\\SA1.WAV.wav   \n",
       "23095  TRAIN/DR8/MRDM0/SI1044.WAV.wav  TRAIN\\\\DR8\\\\MRDM0\\\\SI1044.WAV.wav   \n",
       "23096       TRAIN/DR8/MRDM0/SX245.WAV       TRAIN\\\\DR8\\\\MRDM0\\\\SX245.WAV   \n",
       "23097     TRAIN/DR8/MRDM0/SA2.WAV.wav     TRAIN\\\\DR8\\\\MRDM0\\\\SA2.WAV.wav   \n",
       "23098       TRAIN/DR8/MRDM0/SX335.WAV       TRAIN\\\\DR8\\\\MRDM0\\\\SX335.WAV   \n",
       "\n",
       "      is_converted_audio is_audio is_word_file is_phonetic_file  \\\n",
       "0                   True     True        False            False   \n",
       "6                   True     True        False            False   \n",
       "7                  False     True        False            False   \n",
       "10                  True     True        False            False   \n",
       "13                  True     True        False            False   \n",
       "...                  ...      ...          ...              ...   \n",
       "23088               True     True        False              NaN   \n",
       "23095               True     True        False              NaN   \n",
       "23096              False     True        False              NaN   \n",
       "23097               True     True        False              NaN   \n",
       "23098              False     True        False              NaN   \n",
       "\n",
       "      is_sentence_file  \n",
       "0                False  \n",
       "6                False  \n",
       "7                False  \n",
       "10               False  \n",
       "13               False  \n",
       "...                ...  \n",
       "23088            False  \n",
       "23095            False  \n",
       "23096            False  \n",
       "23097            False  \n",
       "23098            False  \n",
       "\n",
       "[9240 rows x 12 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = dict()\n",
    "\n",
    "for i, row in train_csv.iterrows():\n",
    "    speaker_id = row[\"speaker_id\"]\n",
    "    if speaker_id not in train_data:\n",
    "        train_data[speaker_id] = []\n",
    "    filepath = row[\"path_from_data_dir\"]\n",
    "    if \".wav\" in filepath:\n",
    "        train_data[speaker_id].append(filepath)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "462"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data.keys()) # number of speakers"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "test_csv = pd.read_csv(\"./vox1_test_wav/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "37720"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pairs = []\n",
    "valid_pairs = 0\n",
    "for _, row in test_csv.iterrows():\n",
    "    path1 = \"./vox1_test_wav/wav/\" + row[\"audio_1\"]\n",
    "    path2 = \"./vox1_test_wav/wav/\" + row[\"audio_2\"]\n",
    "    issame = True if row[\"label\"] == 1 else False\n",
    "    if os.path.exists(path1) and os.path.exists(path2):\n",
    "        test_pairs.append((path1, path2, issame))\n",
    "        valid_pairs += 1\n",
    "\n",
    "valid_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# count = 0\n",
    "# for pair in test_pairs:\n",
    "#     if pair[2] == True:\n",
    "#         count += 1\n",
    "# count"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset transformation for training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.utils.data as data\n",
    "\n",
    "def generate_triplets(imgs, num_triplets,n_classes):\n",
    "    def create_indices(_imgs):\n",
    "        inds = dict()\n",
    "        for idx, (img_path,label) in enumerate(_imgs):\n",
    "            if label not in inds:\n",
    "                inds[label] = []\n",
    "            inds[label].append(img_path)\n",
    "        return inds\n",
    "\n",
    "    triplets = []\n",
    "    # Indices = array of labels and each label is an array of indices\n",
    "    indices = create_indices(imgs)\n",
    "\n",
    "    #for x in tqdm(range(num_triplets)):\n",
    "    for x in range(num_triplets):\n",
    "        c1 = np.random.randint(0, n_classes)\n",
    "        c2 = np.random.randint(0, n_classes)\n",
    "        while len(indices[c1]) < 2:\n",
    "            c1 = np.random.randint(0, n_classes)\n",
    "\n",
    "        while c1 == c2:\n",
    "            c2 = np.random.randint(0, n_classes)\n",
    "        if len(indices[c1]) == 2:  # hack to speed up process\n",
    "            n1, n2 = 0, 1\n",
    "        else:\n",
    "            n1 = np.random.randint(0, len(indices[c1]) - 1)\n",
    "            n2 = np.random.randint(0, len(indices[c1]) - 1)\n",
    "            while n1 == n2:\n",
    "                n2 = np.random.randint(0, len(indices[c1]) - 1)\n",
    "        if len(indices[c2]) ==1:\n",
    "            n3 = 0\n",
    "        else:\n",
    "            n3 = np.random.randint(0, len(indices[c2]) - 1)\n",
    "\n",
    "        triplets.append([indices[c1][n1], indices[c1][n2], indices[c2][n3],c1,c2])\n",
    "    return triplets\n",
    "\n",
    "def find_id(data):\n",
    "    speakerids = list(data.keys())\n",
    "    speakerids.sort()\n",
    "    speakerid2id = {speakerids[i]: i for i in range(len(speakerids))}\n",
    "    return speakerids, speakerid2id\n",
    "\n",
    "class DeepSpeakerDataset(data.Dataset):\n",
    "    def __init__(self, data, n_triplets, loader, transform=None):\n",
    "        self.classes, speakerid2id = find_id(data)\n",
    "        imgs = []\n",
    "        for speaker_id in data.keys():\n",
    "            true_id = speakerid2id[speaker_id]\n",
    "            for item in data[speaker_id]:\n",
    "                imgs.append((config.dataroot + \"/\" + item, true_id))\n",
    "\n",
    "        self.imgs = imgs\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        self.n_triplets = n_triplets\n",
    "\n",
    "        print('Generating {} triplets'.format(self.n_triplets))\n",
    "        self.training_triplets = generate_triplets(self.imgs, self.n_triplets,len(self.classes))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        '''\n",
    "        Args:\n",
    "            index: Index of the triplet or the matches - not of a single image\n",
    "\n",
    "        Returns:\n",
    "\n",
    "        '''\n",
    "        def transform(img_path):\n",
    "            \"\"\"Convert image into numpy array and apply transformation\n",
    "               Doing this so that it is consistent with all other datasets\n",
    "               to return a PIL Image.\n",
    "            \"\"\"\n",
    "\n",
    "            img = self.loader(img_path)\n",
    "            # img = np.load(img_path.replace('.wav', '.npy'))\n",
    "            return self.transform(img)\n",
    "\n",
    "        # Get the index of each image in the triplet\n",
    "        a, p, n,c1,c2 = self.training_triplets[index]\n",
    "\n",
    "        # transform images if required\n",
    "        img_a, img_p, img_n = transform(a), transform(p), transform(n)\n",
    "        return img_a, img_p, img_n, c1, c2\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.training_triplets)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "class TestDataset(data.Dataset):\n",
    "    def __init__(self, pairs, loader, transform=None):\n",
    "        self.pairs = pairs\n",
    "        self.loader = loader\n",
    "        self.transform = transform\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        def transform(img_path):\n",
    "            img = self.loader(img_path)\n",
    "            return self.transform(img)       \n",
    "\n",
    "        path1, path2, issame = self.pairs[index]\n",
    "        img1, img2 = self.loader(path1), self.loader(path2)\n",
    "        img1, img2 = transform(path1), transform(path2)\n",
    "        return img1, img2, issame\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.pairs)    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_PREVIOUS_FRAME = 9\n",
    "#NUM_PREVIOUS_FRAME = 13\n",
    "NUM_NEXT_FRAME = 23\n",
    "\n",
    "NUM_FRAMES = NUM_PREVIOUS_FRAME + NUM_NEXT_FRAME\n",
    "USE_LOGSCALE = True\n",
    "USE_DELTA = False\n",
    "USE_SCALE = False\n",
    "SAMPLE_RATE = 16000\n",
    "TRUNCATE_SOUND_FIRST_SECONDS = 0.5\n",
    "FILTER_BANK = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from python_speech_features import fbank, delta\n",
    "\n",
    "import librosa\n",
    "\n",
    "def normalize_frames(m,Scale=True):\n",
    "    if Scale:\n",
    "        return (m - np.mean(m, axis=0)) / (np.std(m, axis=0) + 2e-12)\n",
    "    else:\n",
    "        return (m - np.mean(m, axis=0))\n",
    "\n",
    "def mk_MFB(filename, sample_rate=SAMPLE_RATE,use_delta = USE_DELTA,use_scale = USE_SCALE,use_logscale = USE_LOGSCALE):\n",
    "    audio, sr = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    #audio = audio.flatten()\n",
    "\n",
    "\n",
    "    filter_banks, energies = fbank(audio, samplerate=sample_rate, nfilt=FILTER_BANK, winlen=0.025)\n",
    "\n",
    "    if use_logscale:\n",
    "        filter_banks = 20 * np.log10(np.maximum(filter_banks,1e-5))\n",
    "\n",
    "    if use_delta:\n",
    "        delta_1 = delta(filter_banks, N=1)\n",
    "        delta_2 = delta(delta_1, N=1)\n",
    "\n",
    "        filter_banks = normalize_frames(filter_banks, Scale=use_scale)\n",
    "        delta_1 = normalize_frames(delta_1, Scale=use_scale)\n",
    "        delta_2 = normalize_frames(delta_2, Scale=use_scale)\n",
    "\n",
    "        frames_features = np.hstack([filter_banks, delta_1, delta_2])\n",
    "    else:\n",
    "        filter_banks = normalize_frames(filter_banks, Scale=use_scale)\n",
    "        frames_features = filter_banks\n",
    "\n",
    "\n",
    "\n",
    "    np.save(filename.replace('.wav', '.npy'),frames_features)\n",
    "\n",
    "    return\n",
    "\n",
    "def read_MFB(filename):\n",
    "    #audio, sr = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    #audio = audio.flatten()\n",
    "    audio = np.load(filename.replace('.wav', '.npy'))\n",
    "    return audio\n",
    "\n",
    "class totensor(object):\n",
    "    \"\"\"Rescales the input PIL.Image to the given 'size'.\n",
    "    If 'size' is a 2-element tuple or list in the order of (width, height), it will be the exactly size to scale.\n",
    "    If 'size' is a number, it will indicate the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the exactly size or the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, pic):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            pic (PIL.Image or numpy.ndarray): Image to be converted to tensor.\n",
    "\n",
    "        Returns:\n",
    "            Tensor: Converted image.\n",
    "        \"\"\"\n",
    "        if isinstance(pic, np.ndarray):\n",
    "            # handle numpy array\n",
    "            #img = torch.from_numpy(pic.transpose((0, 2, 1)))\n",
    "            #return img.float()\n",
    "            img = torch.FloatTensor(pic.transpose((0, 2, 1)))\n",
    "            #img = np.float32(pic.transpose((0, 2, 1)))\n",
    "            return img\n",
    "\n",
    "            #img = torch.from_numpy(pic)\n",
    "            # backward compatibility\n",
    "\n",
    "\n",
    "class truncatedinputfromMFB(object):\n",
    "    \"\"\"Rescales the input PIL.Image to the given 'size'.\n",
    "    If 'size' is a 2-element tuple or list in the order of (width, height), it will be the exactly size to scale.\n",
    "    If 'size' is a number, it will indicate the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the exactly size or the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "    def __init__(self, input_per_file=1):\n",
    "\n",
    "        super(truncatedinputfromMFB, self).__init__()\n",
    "        self.input_per_file = input_per_file\n",
    "\n",
    "    def __call__(self, frames_features):\n",
    "\n",
    "        network_inputs = []\n",
    "        num_frames = len(frames_features)\n",
    "        import random\n",
    "\n",
    "        for i in range(self.input_per_file):\n",
    "\n",
    "            j = random.randrange(NUM_PREVIOUS_FRAME, num_frames - NUM_NEXT_FRAME)\n",
    "            if not j:\n",
    "                frames_slice = np.zeros(NUM_FRAMES, FILTER_BANK, 'float64')\n",
    "                frames_slice[0:(frames_features.shape)[0]] = frames_features.shape\n",
    "            else:\n",
    "                frames_slice = frames_features[j - NUM_PREVIOUS_FRAME:j + NUM_NEXT_FRAME]\n",
    "            network_inputs.append(frames_slice)\n",
    "\n",
    "        return np.array(network_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def read_audio(filename, sample_rate=SAMPLE_RATE):\n",
    "    audio, sr = librosa.load(filename, sr=sample_rate, mono=True)\n",
    "    audio = audio.flatten()\n",
    "    return audio\n",
    "class truncatedinput(object):\n",
    "    \"\"\"Rescales the input PIL.Image to the given 'size'.\n",
    "    If 'size' is a 2-element tuple or list in the order of (width, height), it will be the exactly size to scale.\n",
    "    If 'size' is a number, it will indicate the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the exactly size or the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, input):\n",
    "\n",
    "        #min_existing_frames = min(self.libri_batch['raw_audio'].apply(lambda x: len(x)).values)\n",
    "        want_size = int(TRUNCATE_SOUND_FIRST_SECONDS * SAMPLE_RATE)\n",
    "        if want_size > len(input):\n",
    "            output = np.zeros((want_size,))\n",
    "            output[0:len(input)] = input\n",
    "            #print(\"biho check\")\n",
    "            return output\n",
    "        else:\n",
    "            return input[0:want_size]\n",
    "\n",
    "def pre_process_inputs(signal=np.random.uniform(size=32000), target_sample_rate=8000,use_delta = USE_DELTA):\n",
    "    filter_banks, energies = fbank(signal, samplerate=target_sample_rate, nfilt=FILTER_BANK, winlen=0.025)\n",
    "    delta_1 = delta(filter_banks, N=1)\n",
    "    delta_2 = delta(delta_1, N=1)\n",
    "\n",
    "    filter_banks = normalize_frames(filter_banks)\n",
    "    delta_1 = normalize_frames(delta_1)\n",
    "    delta_2 = normalize_frames(delta_2)\n",
    "\n",
    "    if use_delta:\n",
    "        frames_features = np.hstack([filter_banks, delta_1, delta_2])\n",
    "    else:\n",
    "        frames_features = filter_banks\n",
    "    num_frames = len(frames_features)\n",
    "    network_inputs = []\n",
    "    \"\"\"Too complicated\n",
    "    for j in range(c.NUM_PREVIOUS_FRAME, num_frames - c.NUM_NEXT_FRAME):\n",
    "        frames_slice = frames_features[j - c.NUM_PREVIOUS_FRAME:j + c.NUM_NEXT_FRAME]\n",
    "        #network_inputs.append(np.reshape(frames_slice, (32, 20, 3)))\n",
    "        network_inputs.append(frames_slice)\n",
    "        \n",
    "    \"\"\"\n",
    "    import random\n",
    "    j = random.randrange(NUM_PREVIOUS_FRAME, num_frames - NUM_NEXT_FRAME)\n",
    "    frames_slice = frames_features[j - NUM_PREVIOUS_FRAME:j + NUM_NEXT_FRAME]\n",
    "    network_inputs.append(frames_slice)\n",
    "    return np.array(network_inputs)\n",
    "\n",
    "class toMFB(object):\n",
    "    \"\"\"Rescales the input PIL.Image to the given 'size'.\n",
    "    If 'size' is a 2-element tuple or list in the order of (width, height), it will be the exactly size to scale.\n",
    "    If 'size' is a number, it will indicate the size of the smaller edge.\n",
    "    For example, if height > width, then image will be\n",
    "    rescaled to (size * height / width, size)\n",
    "    size: size of the exactly size or the smaller edge\n",
    "    interpolation: Default: PIL.Image.BILINEAR\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, input):\n",
    "\n",
    "        output = pre_process_inputs(input, target_sample_rate=SAMPLE_RATE)\n",
    "        return output"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agent "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimizer init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "def create_optimizer(model, new_lr):\n",
    "    # setup optimizer\n",
    "    if config.optimizer == 'sgd':\n",
    "        optimizer = optim.SGD(model.parameters(), lr=new_lr,\n",
    "                              momentum=0.9, dampening=0.9,\n",
    "                              weight_decay=config.wd)\n",
    "    elif config.optimizer == 'adam':\n",
    "        optimizer = optim.Adam(model.parameters(), lr=new_lr,\n",
    "                               weight_decay=config.wd)\n",
    "    elif config.optimizer == 'adagrad':\n",
    "        optimizer = optim.Adagrad(model.parameters(),\n",
    "                                  lr=new_lr,\n",
    "                                  lr_decay=config.lr_decay,\n",
    "                                  weight_decay=config.wd)\n",
    "    return optimizer\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def evaluate(distances, labels):\n",
    "    thresholds = np.arange(0, 10, 0.001)\n",
    "    fm, tpr, acc = calculate_roc(thresholds, distances, labels)\n",
    "    eer = calculate_eer(thresholds, distances, labels)\n",
    "    return acc, eer\n",
    "\n",
    "def calculate_roc(thresholds, distances, labels):\n",
    "    nrof_thresholds = len(thresholds)\n",
    "\n",
    "    tprs = np.zeros((nrof_thresholds))\n",
    "    fprs = np.zeros((nrof_thresholds))\n",
    "    acc_train = np.zeros((nrof_thresholds))\n",
    "    precisions = np.zeros((nrof_thresholds))\n",
    "    fms = np.zeros((nrof_thresholds))\n",
    "\n",
    "    for threshold_idx, threshold in enumerate(threshold):\n",
    "        tprs[threshold_idx], fprs[threshold_idx], precisions[threshold_idx], fms[threshold_idx], \\\n",
    "            acc_train[threshold_idx] = calculate_accuracy(threshold, distances, labels) \n",
    "\n",
    "    best_id = np.argmax(fms)\n",
    "    best_fm = fms[best_id]\n",
    "    best_tpr = tprs[best_id]\n",
    "    best_acc = acc_train[best_id]\n",
    "\n",
    "    return best_fm, best_tpr, best_acc\n",
    "\n",
    "def calculate_accuracy(threshold, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, threshold)\n",
    "    tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "    fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "    tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "    fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "    acc = float(tp+tn)/dist.size\n",
    "    precision = 0 if (tp+fp==0) else float(tp)/float(tp+fp)\n",
    "    fm = 2*precision*tpr / (precision+tpr+1e-12)\n",
    "    return tpr, fpr, precision, fm, acc\n",
    "\n",
    "def calculate_eer(thresholds, distances, labels):\n",
    "    nrof_thresholds = len(thresholds)\n",
    "\n",
    "    far_train = np.zeros(nrof_thresholds)\n",
    "    frr_train = np.zeros(nrof_thresholds)\n",
    "    eer_id = 0\n",
    "    eer_diff = 100000000\n",
    "    \n",
    "    for thres_id, thres in enumerate(thresholds):\n",
    "        frr_train[thres_id], far_train[thres_id] = calculate_val_far(thres, distance, labels)\n",
    "        tmp = abs(frr_train[thres_id] - far_train[thres_id])\n",
    "        if  tmp < eer_diff:\n",
    "            eer_diff = tmp \n",
    "            eer_id = thres_id\n",
    "\n",
    "    frr, far = frr_train[eer_id], far_train[eer_id]\n",
    "    eer = (frr + far) / 2.0\n",
    "    return eer \n",
    "\n",
    "def calculate_val_far(thres, dist, actual_issame):\n",
    "    predict_issame = np.less(dist, thres)\n",
    "    true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "    false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "    n_same = np.sum(actual_issame)\n",
    "    n_diff = np.sum(np.logical_not(actual_issame))\n",
    "    if n_diff == 0:\n",
    "        n_diff = 1\n",
    "    if n_same == 0:\n",
    "        return 0, 0\n",
    "    val = float(true_accept) / float(n_same)\n",
    "    frr = 1 - val\n",
    "    far = float(false_accept) / float(n_diff)\n",
    "    return frr, far\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from scipy import interpolate\n",
    "\n",
    "\n",
    "# def evaluate(distances, labels):\n",
    "#     # Calculate evaluation metrics\n",
    "#     thresholds = np.arange(0, 30, 0.01)\n",
    "#     tpr, fpr, accuracy, frr = calculate_roc(thresholds, distances, labels)\n",
    "#     # thresholds = np.arange(0, 30, 0.001)\n",
    "#     # val,  far = calculate_val(thresholds, distances, labels, 1e-3)\n",
    "#     val, far = 0,0\n",
    "#     # eer = calculate_eer(fpr, tpr)\n",
    "#     eer = (fpr + frr) / 2.0\n",
    "#     return tpr, fpr, accuracy, val, far, eer\n",
    "\n",
    "\n",
    "# def calculate_roc(thresholds, distances, labels):\n",
    "#     nrof_pairs = min(len(labels), len(distances))\n",
    "#     nrof_thresholds = len(thresholds)\n",
    "\n",
    "#     tprs = np.zeros((nrof_thresholds))\n",
    "#     fprs = np.zeros((nrof_thresholds))\n",
    "#     acc_train = np.zeros((nrof_thresholds))\n",
    "#     frrs = np.zeros(nrof_thresholds)\n",
    "#     accuracy = 0.0\n",
    "\n",
    "#     indices = np.arange(nrof_pairs)\n",
    "\n",
    "#     # Find the best threshold for the fold\n",
    "#     for threshold_idx, threshold in enumerate(thresholds):\n",
    "#         tprs[threshold_idx], fprs[threshold_idx], acc_train[threshold_idx], frrs[threshold_idx] \\\n",
    "#                 = calculate_accuracy(threshold, distances, labels)\n",
    "\n",
    "#     best_threshold_index = np.argmax(acc_train)\n",
    "\n",
    "#     return tprs[best_threshold_index], fprs[best_threshold_index], \\\n",
    "#             acc_train[best_threshold_index], frrs[best_threshold_index]\n",
    "\n",
    "\n",
    "# def calculate_accuracy(threshold, dist, actual_issame):\n",
    "#     predict_issame = np.less(dist, threshold)\n",
    "#     tp = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "#     fp = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "#     tn = np.sum(np.logical_and(np.logical_not(predict_issame), np.logical_not(actual_issame)))\n",
    "#     fn = np.sum(np.logical_and(np.logical_not(predict_issame), actual_issame))\n",
    "\n",
    "#     tpr = 0 if (tp+fn==0) else float(tp) / float(tp+fn)\n",
    "#     fpr = 0 if (fp+tn==0) else float(fp) / float(fp+tn)\n",
    "#     acc = float(tp+tn)/dist.size\n",
    "\n",
    "#     frr = 0 if (tp+fn==0) else float(fn) / float(tp+fn)\n",
    "#     return tpr, fpr, acc, frr\n",
    "\n",
    "\n",
    "# def calculate_val(thresholds, distances, labels, far_target=0.1):\n",
    "#     nrof_pairs = min(len(labels), len(distances))\n",
    "#     nrof_thresholds = len(thresholds)\n",
    "\n",
    "#     indices = np.arange(nrof_pairs)\n",
    "\n",
    "\n",
    "#     # Find the threshold that gives FAR = far_target\n",
    "#     far_train = np.zeros(nrof_thresholds)\n",
    "\n",
    "#     for threshold_idx, threshold in enumerate(thresholds):\n",
    "#         _, far_train[threshold_idx] = calculate_val_far(threshold, distances, labels)\n",
    "#     if np.max(far_train)>=far_target:\n",
    "#         f = interpolate.interp1d(far_train, thresholds, kind='slinear')\n",
    "#         threshold = f(far_target)\n",
    "#     else:\n",
    "#         threshold = 0.0\n",
    "\n",
    "#     val, far = calculate_val_far(threshold, distances, labels)\n",
    "\n",
    "\n",
    "#     return val, far\n",
    "\n",
    "\n",
    "# def calculate_val_far(threshold, dist, actual_issame):\n",
    "#     predict_issame = np.less(dist, threshold)\n",
    "#     true_accept = np.sum(np.logical_and(predict_issame, actual_issame))\n",
    "#     false_accept = np.sum(np.logical_and(predict_issame, np.logical_not(actual_issame)))\n",
    "#     n_same = np.sum(actual_issame)\n",
    "#     n_diff = np.sum(np.logical_not(actual_issame))\n",
    "#     if n_diff == 0:\n",
    "#         n_diff = 1\n",
    "#     if n_same == 0:\n",
    "#         return 0,0\n",
    "#     val = float(true_accept) / float(n_same)\n",
    "#     far = float(false_accept) / float(n_diff)\n",
    "#     return val, far"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm \n",
    "\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def train(train_loader, model, optimizer, epoch):\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "    labels, distances = [], []\n",
    "\n",
    "    # pbar = tqdm(enumerate(train_loader))\n",
    "    pbar = enumerate(train_loader)\n",
    "    for batch_idx, (data_a, data_p, data_n, label_p, label_n) in tqdm(enumerate(train_loader)):\n",
    "        #print(\"on training{}\".format(epoch))\n",
    "        data_a, data_p, data_n = data_a.cuda(), data_p.cuda(), data_n.cuda()\n",
    "        data_a, data_p, data_n = Variable(data_a), Variable(data_p), \\\n",
    "                                 Variable(data_n)\n",
    "\n",
    "        # compute output\n",
    "        out_a, out_p, out_n = model(data_a), model(data_p), model(data_n)\n",
    "\n",
    "\n",
    "        if epoch > config.min_softmax_epoch:\n",
    "            triplet_loss = TripletMarginLoss(config.margin).forward(out_a, out_p, out_n)\n",
    "            loss = triplet_loss\n",
    "            # compute gradient and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # logger.log_value('selected_triplet_loss', triplet_loss.data[0]).step()\n",
    "            # #logger.log_value('selected_cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "            # logger.log_value('selected_total_loss', loss.data[0]).step()\n",
    "\n",
    "            # if batch_idx % config.log_interval == 0:\n",
    "            #     pbar.set_description(\n",
    "            #         'Train Epoch: {:3d} [{:8d}/{:8d} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            #             epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "            #             100. * batch_idx / len(train_loader),\n",
    "            #             loss.data[0]))\n",
    "\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                print('Train Epoch: {:3d} [{:8d}/{:8d} ({:3.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                            epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                            100. * batch_idx / len(train_loader),\n",
    "                            loss.data.item()))\n",
    "\n",
    "            # two times\n",
    "            dists = l2_dist.forward(out_a,out_n) #torch.sqrt(torch.sum((out_a - out_n) ** 2, 1))  # euclidean distance\n",
    "            distances.append(dists.data.cpu().numpy())\n",
    "            labels.append(np.zeros(dists.size(0)))\n",
    "\n",
    "\n",
    "            dists = l2_dist.forward(out_a,out_p) #torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "            distances.append(dists.data.cpu().numpy())\n",
    "            labels.append(np.ones(dists.size(0)))\n",
    "\n",
    "        else:\n",
    "        # Choose the hard negatives\n",
    "            d_p = l2_dist.forward(out_a, out_p)\n",
    "            d_n = l2_dist.forward(out_a, out_n)\n",
    "            all = (d_n - d_p < config.margin).cpu().data.numpy().flatten()\n",
    "\n",
    "            # log loss value for mini batch.\n",
    "            total_coorect = np.where(all == 0)\n",
    "            # logger.log_value('Minibatch Train Accuracy', len(total_coorect[0]))\n",
    "\n",
    "            total_dist = (d_n - d_p).cpu().data.numpy().flatten()\n",
    "            # logger.log_value('Minibatch Train distance', np.mean(total_dist))\n",
    "\n",
    "            hard_triplets = np.where(all == 1)\n",
    "            if len(hard_triplets[0]) == 0:\n",
    "                continue\n",
    "            out_selected_a = Variable(torch.from_numpy(out_a.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "            out_selected_p = Variable(torch.from_numpy(out_p.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "            out_selected_n = Variable(torch.from_numpy(out_n.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "\n",
    "            selected_data_a = Variable(torch.from_numpy(data_a.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "            selected_data_p = Variable(torch.from_numpy(data_p.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "            selected_data_n = Variable(torch.from_numpy(data_n.cpu().data.numpy()[hard_triplets]).cuda())\n",
    "\n",
    "            selected_label_p = torch.from_numpy(label_p.cpu().numpy()[hard_triplets])\n",
    "            selected_label_n= torch.from_numpy(label_n.cpu().numpy()[hard_triplets])\n",
    "            triplet_loss = TripletMarginLoss(config.margin).forward(out_selected_a, out_selected_p, out_selected_n)\n",
    "\n",
    "            cls_a = model.forward_classifier(selected_data_a)\n",
    "            cls_p = model.forward_classifier(selected_data_p)\n",
    "            cls_n = model.forward_classifier(selected_data_n)\n",
    "\n",
    "            criterion = nn.CrossEntropyLoss()\n",
    "            predicted_labels = torch.cat([cls_a,cls_p,cls_n])\n",
    "            true_labels = torch.cat([Variable(selected_label_p.cuda()),Variable(selected_label_p.cuda()),Variable(selected_label_n.cuda())])\n",
    "\n",
    "            cross_entropy_loss = criterion(predicted_labels.cuda(),true_labels.cuda())\n",
    "\n",
    "            loss = cross_entropy_loss + triplet_loss * config.loss_ratio\n",
    "            # compute gradient and update weights\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "\n",
    "            # log loss value for hard selected sample\n",
    "            # logger.log_value('selected_triplet_loss', triplet_loss.data[0]).step()\n",
    "            # logger.log_value('selected_cross_entropy_loss', cross_entropy_loss.data[0]).step()\n",
    "            # logger.log_value('selected_total_loss', loss.data[0]).step()\n",
    "            # if batch_idx % config.log_interval == 0:\n",
    "            #     pbar.set_description(\n",
    "            #         'Train Epoch: {:3d} [{:8d}/{:8d} ({:3.0f}%)]\\tLoss: {:.6f} \\t # of Selected Triplets: {:4d}'.format(\n",
    "            #             epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "            #             100. * batch_idx / len(train_loader),\n",
    "            #             loss.data[0],len(hard_triplets[0])))\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                print('Train Epoch: {:3d} [{:8d}/{:8d} ({:3.0f}%)]\\tLoss: {:.6f} \\t # of Selected Triplets: {:4d}'.format(\n",
    "                    epoch, batch_idx * len(data_a), len(train_loader.dataset),\n",
    "                    100. * batch_idx / len(train_loader),\n",
    "                    loss.data.item(),len(hard_triplets[0])))\n",
    "            \n",
    "            dists = l2_dist.forward(out_selected_a,out_selected_n) #torch.sqrt(torch.sum((out_a - out_n) ** 2, 1))  # euclidean distance\n",
    "            distances.append(dists.data.cpu().numpy())\n",
    "            labels.append(np.zeros(dists.size(0)))\n",
    "\n",
    "\n",
    "            dists = l2_dist.forward(out_selected_a,out_selected_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "            distances.append(dists.data.cpu().numpy())\n",
    "            labels.append(np.ones(dists.size(0)))\n",
    "\n",
    "\n",
    "    #accuracy for hard selected sample, not all sample.\n",
    "    labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "    distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "\n",
    "    accuracy, eer = evaluate(distances,labels)\n",
    "    print('\\33[91mTrain set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "    # logger.log_value('Train Accuracy', np.mean(accuracy))\n",
    "\n",
    "    # do checkpointing\n",
    "    torch.save({'epoch': epoch + 1, 'state_dict': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict()},\n",
    "               '{}/checkpoint_{}.pth'.format(\"save_models\", epoch))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(test_loader, model, epoch):\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    labels, distances = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # pbar = enumerate(test_loader)\n",
    "        pbar = tqdm(enumerate(test_loader))\n",
    "        for batch_idx, (data_a, data_p, label) in pbar:\n",
    "            current_sample = data_a.size(0)\n",
    "            data_a = data_a.resize_(config.test_input_per_file *current_sample, 1, data_a.size(2), data_a.size(3))\n",
    "            data_p = data_p.resize_(config.test_input_per_file *current_sample, 1, data_a.size(2), data_a.size(3))\n",
    "            if config.cuda:\n",
    "                data_a, data_p = data_a.cuda(), data_p.cuda()\n",
    "            # data_a, data_p, label = Variable(data_a, volatile=True), \\\n",
    "            #                         Variable(data_p, volatile=True), Variable(label)\n",
    "\n",
    "            # compute output\n",
    "            out_a, out_p = model(data_a), model(data_p)\n",
    "            dists = l2_dist.forward(out_a,out_p)#torch.sqrt(torch.sum((out_a - out_p) ** 2, 1))  # euclidean distance\n",
    "            dists = dists.data.cpu().numpy()\n",
    "            dists = dists.reshape(current_sample,config.test_input_per_file).mean(axis=1)\n",
    "            distances.append(dists)\n",
    "            labels.append(label.data.cpu().numpy())\n",
    "\n",
    "            if batch_idx % config.log_interval == 0:\n",
    "                pbar.set_description('Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                    epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                    100. * batch_idx / len(test_loader)))\n",
    "                # print('Test Epoch: {} [{}/{} ({:.0f}%)]'.format(\n",
    "                #     epoch, batch_idx * len(data_a), len(test_loader.dataset),\n",
    "                #     100. * batch_idx / len(test_loader)))\n",
    "        \n",
    "        labels = np.array([sublabel for label in labels for sublabel in label])\n",
    "        distances = np.array([subdist for dist in distances for subdist in dist])\n",
    "        print(labels)\n",
    "        print(distances)\n",
    "        #print(\"distance {.8f}\".format(distances))\n",
    "        #print(\"distance {.1f}\".format(labels))\n",
    "        accuracy,eer = evaluate(distances,labels)\n",
    "        print('\\33[91mTest set: Accuracy: {:.8f}\\n\\33[0m'.format(np.mean(accuracy)))\n",
    "        print(f\"Test epoch {epoch}: eer={eer}\")\n",
    "        # print(f\"Test epoch {epoch}: Accuracy {np.mean(accuracy)}\")\n",
    "        # logger.log_value('Test Accuracy', np.mean(accuracy))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # convert wav file to npy \n",
    "# for key in train_data.keys():\n",
    "#     for file in train_data[key]:\n",
    "#         mk_MFB(\"./data/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file1, file2, _ in test_pairs:\n",
    "#     mk_MFB(file1)\n",
    "#     mk_MFB(file2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating 100000 triplets\n"
     ]
    }
   ],
   "source": [
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "\n",
    "file_loader = read_MFB\n",
    "transform = transforms.Compose([\n",
    "    truncatedinputfromMFB(),\n",
    "    totensor()\n",
    "])\n",
    "transform_T = transforms.Compose([\n",
    "    truncatedinputfromMFB(input_per_file=config.test_input_per_file),\n",
    "    totensor()\n",
    "])\n",
    "train_dir = DeepSpeakerDataset(train_data, config.n_triplets, file_loader, transform)\n",
    "test_dir = TestDataset(test_pairs, file_loader, transform_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_T = transforms.Compose([\n",
    "    truncatedinputfromMFB(input_per_file=config.test_input_per_file),\n",
    "    totensor()\n",
    "])\n",
    "test_dir = TestDataset(test_pairs, file_loader, transform_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hi I'm tired\n"
     ]
    }
   ],
   "source": [
    "model = DeepSpeakerModel(embedding_size = config.embedding_size,\n",
    "                    num_classes=len(train_dir.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DeepSpeakerModel(\n",
       "  (model): myResNet(\n",
       "    (relu): ReLU (inplace)\n",
       "    (conv1): Conv2d(1, 64, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer1): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU (inplace)\n",
       "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv2): Conv2d(64, 128, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer2): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU (inplace)\n",
       "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv3): Conv2d(128, 256, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer3): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU (inplace)\n",
       "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (conv4): Conv2d(256, 512, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), bias=False)\n",
       "    (bn4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (layer4): Sequential(\n",
       "      (0): BasicBlock(\n",
       "        (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "        (relu): ReLU (inplace)\n",
       "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (avgpool): AdaptiveAvgPool2d(output_size=(None, 1))\n",
       "    (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
       "    (classifier): Linear(in_features=512, out_features=462, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "l2_dist = PairwiseDistance(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = create_optimizer(model, config.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = {'num_workers': 0, 'pin_memory': True} if config.cuda else {}\n",
    "train_loader = DataLoader(train_dir, batch_size=config.batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = DataLoader(test_dir, batch_size=config.test_batch_size, shuffle=False, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = config.start_epoch\n",
    "end = start + config.epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Epoch: 1 [113088/37720 (100%)]: : 590it [01:19,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ True False  True ... False  True False]\n",
      "[4.3891478 4.2811627 4.1075993 ... 3.7663    4.1351953 3.966806 ]\n",
      "\u001b[91mTest set: Accuracy: 0.53698303\n",
      "\u001b[0m\n",
      "Test epoch 1: tpr=0.47741251325556733 fpr=0.40344644750795333 eer=0.463016967126193 far=0\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(start, end):\n",
    "    # train(train_loader, model, optimizer, epoch)\n",
    "    test(test_loader, model, epoch)\n",
    "    break"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# END"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hh_env",
   "language": "python",
   "name": "hh_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "233e438fb97c1eec7746b2c3667562cd1696e2c3e02bd537ea2b6b4c82077020"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
